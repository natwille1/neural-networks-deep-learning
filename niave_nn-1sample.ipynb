{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise weights and bias matrices\n",
    "\n",
    "The neural network will consist of the following layers:\n",
    "\n",
    "Input layer = 3 nodes \n",
    "\n",
    "\n",
    "Hidden layer = 2 nodes\n",
    "\n",
    "\n",
    "Output layer = 1 node \n",
    "\n",
    "Therefore, the weight and bias matrices must be initialised to suit the dimensions of each layer. \n",
    "\n",
    "The input data has three features: x1, x2, x3. To keep things simple, we wil work with one data sample. The input matrix is therefore a 1 x 3 matrix (features as columns). \n",
    "\n",
    "The weight matrix for the first layer will feed the input data (1 x 3 matrix) to the hidden layer (2 nodes). Therefore, the dimensions of the weight matrix will be 3 x 2 matrix. 3 rows for each feature, and 2 columns - one for each node in the hidden layer. \n",
    "\n",
    "The bias matrix will be a 1 x 2 matrix - one bias for each node in the hidden layer (2 nodes).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:  3\n",
      "weights:  (3, 2)\n",
      "bias:  (1, 2)\n",
      "inputs:  (1, 3)\n",
      "outputs:  (1, 1)\n",
      "weights:  [array([[ 0.67408638,  0.62548787],\n",
      "       [ 0.44157195, -0.12582172],\n",
      "       [ 0.44210482, -0.33442851]]), array([[-0.67978993],\n",
      "       [-1.14167227]])]\n"
     ]
    }
   ],
   "source": [
    "# Example input - n = 1\n",
    "inp = [np.array([1,3,4])]\n",
    "out = np.array([[1]])\n",
    "inp = np.vstack(inp)\n",
    "\n",
    "# input layer dimensions have to be equal to the number of features from the input data set \n",
    "sizes = [3,2,1]\n",
    "num_layers = len(sizes)\n",
    "\n",
    "# initialise weights and biases to random values\n",
    "biases = [np.random.randn(1,y) for y in sizes[1:]]\n",
    "weights = [np.random.randn(x,y) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "layers = len(sizes)\n",
    "\n",
    "print(\"layers: \", num_layers)\n",
    "print(\"weights: \", weights[0].shape)\n",
    "print(\"bias: \", biases[0].shape)\n",
    "print(\"inputs: \", inp.shape)\n",
    "print(\"outputs: \", out.shape)\n",
    "\n",
    "print(\"weights: \", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nabla_w:  [array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]), array([[0.],\n",
      "       [0.]])]\n",
      "activations:  [[1 3 4]]\n",
      "Updated activations:  [[0.02435395 0.94804618]]\n",
      "activations:  [[0.02435395 0.94804618]]\n",
      "Updated activations:  [[0.6263032]]\n",
      "final activations:  [array([[1, 3, 4]]), array([[0.02435395, 0.94804618]]), array([[0.6263032]])]\n",
      "final zs:  [array([[ 3.69040597, -2.90404798]]), array([[-0.51638981]])]\n"
     ]
    }
   ],
   "source": [
    "def feedforward(a, w, b):\n",
    "    z = np.dot(a,w) + b \n",
    "    anew = 1 / (1 + np.exp(z))\n",
    "    return anew\n",
    "\n",
    "def sigmoid(z):\n",
    "    a = 1 / (1 + np.exp(z))\n",
    "    return a\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def cost_derivative(a, y):\n",
    "    e = a - y \n",
    "    return e \n",
    "\n",
    "# initialise inputs \n",
    "a = inp\n",
    "activations = [a]\n",
    "zs = []\n",
    "nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "print(\"nabla_w: \", nabla_w)\n",
    "\n",
    "# feedforward \n",
    "# for each sample, feedforward through the layers - results in 1 outputs \n",
    "for w, b in zip(weights, biases):\n",
    "    print(\"activations: \", a)\n",
    "    z = np.dot(a, w) + b\n",
    "    zs.append(z)\n",
    "    a = sigmoid(z)\n",
    "    print(\"Updated activations: \", a)\n",
    "    activations.append(a)\n",
    "\n",
    "print(\"final activations: \", activations)\n",
    "print(\"final zs: \", zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activations list:  [array([[1, 3, 4]]), array([[0.02435395, 0.94804618]]), array([[0.6263032]])]\n",
      "z list:  [array([[ 3.69040597, -2.90404798]]), array([[-0.51638981]])]\n",
      "layer: 1, activations: [[1 3 4]]\n",
      "layer: 2, activations: [[0.02435395 0.94804618]]\n",
      "z:  [[-0.51638981]]\n",
      "out:  [[1]]\n",
      "delta error:  [[-0.0874628]]\n",
      "bias error:  [[-0.0874628]]\n",
      "weight error:  [[-0.00213006]\n",
      " [-0.08291878]]\n",
      "bias error:  [[0.00141273 0.00491826]]\n",
      "weight error:  [[0.00141273 0.00491826]\n",
      " [0.0042382  0.01475479]\n",
      " [0.00565093 0.01967306]]\n"
     ]
    }
   ],
   "source": [
    "# backward propagation \n",
    "# out = output defined in the first cell \n",
    "print(\"activations list: \", activations)\n",
    "print(\"z list: \", zs)\n",
    "for layer in range(len(sizes) - 1):\n",
    "    print(\"layer: %s, activations: %s\" % (layer + 1, activations[layer]))\n",
    "\n",
    "# 1. Error of the output activations \n",
    "print(\"z: \", zs[-1])\n",
    "print(\"out: \", out)\n",
    "\n",
    "# calculate the error of the output layer to initiate backpropagation\n",
    "delta = cost_derivative(activations[-1], out) * sigmoid_prime(zs[-1])\n",
    "print(\"delta error: \", delta)\n",
    "\n",
    "# 2. Error of layer 3 output activations wrto the layer 2 bias \n",
    "nabla_b[-1] = delta \n",
    "print(\"bias error: \", nabla_b[-1])\n",
    "\n",
    "# 3. Error of layer 3 output activations wrto the layer 2 weights \n",
    "# error wrto weights in layer L is calculated by getting the activations of layer L-1 \n",
    "# which is treated as the input to weights in layer L (Ain*weights = error out)\n",
    "# print(\"activations from layer -2 (layer 1) \", activations[-2])\n",
    "# print(\"transpose: \", activations[-2].transpose())\n",
    "nb_weight = np.dot(activations[-2].transpose(), delta)\n",
    "nabla_w[-1] = nb_weight\n",
    "print(\"weight error: \", nb_weight)\n",
    "\n",
    "# 4. Error of the layer 2 activations wrto to the layer 1 (input layer) bias\n",
    "delta_2 = np.dot(delta, weights[-1].transpose()) * sigmoid_prime(zs[-2])\n",
    "nabla_b[-2] = delta_2\n",
    "print(\"bias error: \", nabla_b[-2])\n",
    "\n",
    "# 5. Error of layer 2 activations wrto layer 1 weights\n",
    "nabla_w[-2] = np.dot(activations[-3].transpose(), delta_2)\n",
    "print(\"weight error: \", nabla_w[-2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated weights: \n",
      " [array([[ 0.67338002,  0.62302873],\n",
      "       [ 0.43945285, -0.13319912],\n",
      "       [ 0.43927936, -0.34426504]]), array([[-0.6787249 ],\n",
      "       [-1.10021288]])] \n",
      "difference: \n",
      " [array([[-0.00070637, -0.00245913],\n",
      "       [-0.0021191 , -0.0073774 ],\n",
      "       [-0.00282546, -0.00983653]]), array([[0.00106503],\n",
      "       [0.04145939]])]\n",
      "updated biases: \n",
      " [array([[-0.0775219 , -1.81681577]]), array([[0.62625519]])] \n",
      "difference: \n",
      " [array([[-0.00070637, -0.00245913]]), array([[0.0437314]])]\n"
     ]
    }
   ],
   "source": [
    "# 4. Adjust the weights and biases through gradient descent \n",
    "eta = 0.5 # learning rate \n",
    "# #delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "\n",
    "updated_weights = [w-(eta/len(inp))*nw for w, nw in zip(weights, nabla_w)]\n",
    "updated_biases = [b-(eta/len(inp))*nb for b, nb in zip(biases, nabla_b)]\n",
    "\n",
    "diff_weights = [new - old for new, old in zip(updated_weights, weights)]\n",
    "diff_biases = [new - old for new, old in zip(updated_biases, biases)]\n",
    "#diff_biases = updated_biases - biases\n",
    "\n",
    "print(\"updated weights: \\n\", updated_weights, \"\\ndifference: \\n\", diff_weights)\n",
    "print(\"updated biases: \\n\", updated_biases, \"\\ndifference: \\n\", diff_biases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
